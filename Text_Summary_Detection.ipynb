{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danusha2004/AI_Text_Summarize/blob/main/Text_Summary_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYdan8zc12aN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate sentence-transformers spacy nltk networkx streamlit matplotlib\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etrJwBel4mvQ"
      },
      "outputs": [],
      "source": [
        "!python -m nltk.downloader punkt stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSDmfuh54x6s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB2ZEpYMBgFI"
      },
      "source": [
        "# Preprocessing Functions\n",
        "Text cleaning, tokenization, and preparation for summarization models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pHzMZipEYBm"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyWphWkTFWSo"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxO7xm3-FmZu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def split_sentences(text):\n",
        "    text = clean_text(text)\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "# Test\n",
        "sample = \"Hello! This is a test. Let's see if it works.\"\n",
        "split_sentences(sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKBB_YK_GAvK"
      },
      "source": [
        "# 3. Extractive Summarization - TextRank\n",
        "Using TextRank algorithm to select the most important sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjSb4RLcF-Ut"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import networkx as nx\n",
        "\n",
        "def textrank_summary(text, n_sentences=3):\n",
        "    sentences = sent_tokenize(text)\n",
        "    if len(sentences) <= n_sentences:\n",
        "        return \" \".join(sentences)\n",
        "\n",
        "    tfidf = TfidfVectorizer().fit_transform(sentences)\n",
        "    sim_matrix = (tfidf * tfidf.T).toarray()\n",
        "\n",
        "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
        "    scores = nx.pagerank_numpy(nx_graph)\n",
        "\n",
        "    ranked = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "    top_sentences = [s for _, s in ranked[:n_sentences]]\n",
        "\n",
        "    return \" \".join(top_sentences)\n",
        "\n",
        "# Test\n",
        "text = \"\"\"Natural language processing (NLP) is a subfield of AI that deals with\n",
        "the interaction between computers and humans using natural language.\"\"\"\n",
        "print(textrank_summary(text, 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmmLKevvGLVP"
      },
      "source": [
        "# 4. Abstractive Summarization\n",
        "Using pre-trained transformer models like BART and T5 to generate summaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeGcuspoGM49"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "text = \"\"\"The history of NLP dates back to the 1950s, when computer scientists first began exploring\n",
        "how machines could process and understand human language. The 1950s marked the dawn of modern natural language processing (NLP) as a formal field,\n",
        "emerging from broader efforts in artificial intelligence and computational linguistics.\n",
        "Several pioneering computer scientists contributed foundational ideas and experiments that shaped the trajectory of NLP.\n",
        "Natural Language Processing (NLP) is a field at the intersection of linguistics and computer science that focuses on enabling machines to understand,\n",
        "interpret, and generate human language. It leverages machine learning (ML) to learn patterns from large text corpora,\n",
        "enabling tasks like sentiment analysis, named entity recognition, and machine translation.\n",
        "Deep learning (DL), a subset of ML, uses neural networks with many layers to model complex language representations,\n",
        "driving state-of-the-art performance in areas such as language modeling, question answering, and conversational agents.\n",
        "Together, NLP, ML, and DL empower systems to parse syntax, capture semantics, and produce fluent, contextually relevant language,\n",
        "while continually benefiting from advances in data, computation, and algorithms.\"\"\"\n",
        "summary = summarizer(text, max_length=60, min_length=20, do_sample=False)\n",
        "\n",
        "print(\"Generated Summary:\", summary[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U834Mkk2Gj4k"
      },
      "source": [
        "# 6. Evaluation with ROUGE\n",
        "Evaluate the quality of generated summaries using ROUGE metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2xTYi4aGwSx"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qt2AqTk-fu4"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4hsJKiRHRPz"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "# Load ROUGE metric\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# Human-written reference (original text)\n",
        "references = [\"\"\"The history of NLP dates back to the 1950s, when computer scientists first began exploring\n",
        "how machines could process and understand human language. The 1950s marked the dawn of modern natural language processing (NLP) as a formal field,\n",
        "emerging from broader efforts in artificial intelligence and computational linguistics.\n",
        "Several pioneering computer scientists contributed foundational ideas and experiments that shaped the trajectory of NLP.\n",
        "Natural Language Processing (NLP) is a field at the intersection of linguistics and computer science that focuses on enabling machines to understand,\n",
        "interpret, and generate human language. It leverages machine learning (ML) to learn patterns from large text corpora,\n",
        "enabling tasks like sentiment analysis, named entity recognition, and machine translation.\n",
        "Deep learning (DL), a subset of ML, uses neural networks with many layers to model complex language representations,\n",
        "driving state-of-the-art performance in areas such as language modeling, question answering, and conversational agents.\n",
        "Together, NLP, ML, and DL empower systems to parse syntax, capture semantics, and produce fluent, contextually relevant language,\n",
        "while continually benefiting from advances in data, computation, and algorithms.\"\"\"]\n",
        "\n",
        "# Model-generated summary\n",
        "predictions = [\"The 1950s marked the dawn of modern natural language processing (NLP) as a formal field. \"\n",
        "               \"NLP leverages machine learning (ML) to learn patterns from large text corpora. \"\n",
        "               \"Deep learning (DL), a subset of ML, uses neural networks with many layers to model complex.\"]\n",
        "\n",
        "# Compute ROUGE scores\n",
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFF1Dm2cHrkg"
      },
      "source": [
        "# 7. Visualization\n",
        "Plot ROUGE scores for different summarization models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bw_kAx6HtBZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "scores = {'rouge1': 0.85, 'rouge2': 0.67, 'rougeL': 0.80}\n",
        "plt.bar(scores.keys(), scores.values())\n",
        "plt.title(\"ROUGE Evaluation Scores\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcwasK19ITKA"
      },
      "source": [
        "# 5. Fine-Tuning the Abstractive Model\n",
        "Train BART or T5 on a custom dataset or a public dataset like CNN/DailyMail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj81QhJgIUNm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "print(dataset)\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "print(dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Rs7V23JJVR0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSGfMZNVJwZg"
      },
      "outputs": [],
      "source": [
        "# -------- Configuration --------\n",
        "model_name = \"facebook/bart-base\"  # or \"t5-base\"\n",
        "dataset_name = \"cnn_dailymail\"\n",
        "dataset_config = \"3.0.0\"\n",
        "text_column = \"article\"     # source column for CNN/DailyMail\n",
        "summary_column = \"highlights\"  # target column (reference summary)\n",
        "max_input_length = 256\n",
        "max_target_length = 64\n",
        "batch_size = 2\n",
        "num_train_epochs = 1\n",
        "output_dir = \"./bart_base_summarization_fast\"\n",
        "seed = 42\n",
        "# -------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4csEa6OdN3k7"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(dataset_name, dataset_config)\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[text_column]\n",
        "    targets = examples[summary_column]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"  # ensure fixed size to speed up data collator on small runs\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets,\n",
        "            max_length=max_target_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sey_CP0ls6rX"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "hi3lCQCKabHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvIXP4ORu9zA"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1. Install Required Packages\n",
        "# =========================\n",
        "!pip install transformers datasets evaluate nltk\n",
        "\n",
        "# =========================\n",
        "# 2. Import Libraries\n",
        "# =========================\n",
        "import numpy as np\n",
        "import nltk\n",
        "import evaluate\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments\n",
        ")\n",
        "\n",
        "# Download punkt for sentence tokenization\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# =========================\n",
        "# 3. Load Dataset\n",
        "# =========================\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "# =========================\n",
        "# 4. Load Tokenizer and Model\n",
        "# =========================\n",
        "model_name = \"facebook/bart-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# =========================\n",
        "# 5. Preprocessing Function\n",
        "# =========================\n",
        "text_column = \"article\"\n",
        "summary_column = \"highlights\"\n",
        "max_input_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        examples[text_column],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Tokenize labels (summaries)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples[summary_column],\n",
        "            max_length=max_target_length,\n",
        "            truncation=True\n",
        "        )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# =========================\n",
        "# 6. Tokenize Dataset\n",
        "# =========================\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# =========================\n",
        "# 7. Small Subset for Quick Training\n",
        "# =========================\n",
        "small_train_dataset = tokenized_datasets[\"train\"].select(range(1000))\n",
        "small_val_dataset = tokenized_datasets[\"validation\"].select(range(200))\n",
        "\n",
        "# =========================\n",
        "# 8. Data Collator\n",
        "# =========================\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# =========================\n",
        "# 9. Define ROUGE Evaluation Metric\n",
        "# =========================\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [p.strip() for p in preds]\n",
        "    labels = [l.strip() for l in labels]\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(p)) for p in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(l)) for l in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    generated_ids, label_ids = eval_pred\n",
        "    preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
        "    labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    preds, labels = postprocess_text(preds, labels)\n",
        "    result = rouge.compute(predictions=preds, references=labels, use_stemmer=True)\n",
        "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
        "\n",
        "    # Track average generated length\n",
        "    pred_lens = [np.count_nonzero(g != tokenizer.pad_token_id) for g in generated_ids]\n",
        "    result[\"gen_len\"] = float(np.mean(pred_lens))\n",
        "    return result\n",
        "\n",
        "# =========================\n",
        "# 10. Training Configuration\n",
        "# =========================\n",
        "output_dir = \"./bart_summarizer\"\n",
        "batch_size = 2\n",
        "seed = 42\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    predict_with_generate=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=1,          # keep it low for testing\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    dataloader_num_workers=2,\n",
        "    seed=seed,\n",
        "    remove_unused_columns=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 11. Initialize Trainer\n",
        "# =========================\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 12. Train Model\n",
        "# =========================\n",
        "trainer.train()\n",
        "\n",
        "# =========================\n",
        "# 13. Save Model & Tokenizer\n",
        "# =========================\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# =========================\n",
        "# 14. Evaluate Model\n",
        "# =========================\n",
        "metrics = trainer.evaluate(eval_dataset=small_val_dataset)\n",
        "print(\"Evaluation Metrics:\", metrics)\n",
        "\n",
        "# =========================\n",
        "# 15. Generate Summaries on Test Data\n",
        "# =========================\n",
        "test_samples = dataset[\"test\"].select(range(2))\n",
        "inputs = [ex[text_column] for ex in test_samples]\n",
        "\n",
        "inputs_tokenized = tokenizer(\n",
        "    inputs,\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "input_ids = inputs_tokenized[\"input_ids\"].to(device)\n",
        "attention_mask = inputs_tokenized[\"attention_mask\"].to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=128,\n",
        "    num_beams=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "# =========================\n",
        "# 16. Display Results\n",
        "# =========================\n",
        "for i, gen in enumerate(generated_texts):\n",
        "    print(f\"\\n=== Example {i+1} ===\")\n",
        "    print(\"Article snippet:\", inputs[i][:500].replace(\"\\n\", \" \"))\n",
        "    print(\"Generated summary:\", gen)\n",
        "    print(\"Reference summary:\", test_samples[i][summary_column])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download punkt and punkt_tab\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "id": "ragWGu1Af2uZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPIHRRLjUbOUr/MxyoYfXny",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}